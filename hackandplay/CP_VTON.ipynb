{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CP-VTON.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9MihU9xyINH",
        "outputId": "1a407d7e-be1a-4b40-ad90-3cc0eafa6f5e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar 14 22:40:44 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea5hNpiSAaRY",
        "outputId": "fce87b01-5a8e-4eaf-9be1-8066edf6522d"
      },
      "source": [
        "!git clone https://github.com/ZipBomb/cp-vton"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cp-vton'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 127 (delta 0), reused 1 (delta 0), pack-reused 123\u001b[K\n",
            "Receiving objects: 100% (127/127), 816.75 KiB | 20.42 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwBpRRo2Pgbj",
        "outputId": "73083391-3d6f-4d2f-eeb8-df4acc67943e"
      },
      "source": [
        "!cd cp-vton && python data_download.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] Downloading data...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWzn2Nvgy4ge",
        "outputId": "980f84fe-ab2e-497f-ffc6-de18c60683dc"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 15.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 12.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 12.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (54.0.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZgCCjUoQB9y",
        "outputId": "fda0ead7-4d12-41ba-f5f3-f464853e51e6"
      },
      "source": [
        "!cd cp-vton && python train.py --name gmm_train_new --stage GMM --workers 4 --save_count 5000 --shuffle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=4, checkpoint='', checkpoint_dir='checkpoints', data_list='train_pairs.txt', datamode='train', dataroot='data', decay_step=100000, display_count=20, fine_height=256, fine_width=192, gpu_ids='', grid_size=5, keep_step=100000, lr=0.0001, name='gmm_train_new', radius=5, save_count=5000, shuffle=True, stage='GMM', tensorboard_dir='tensorboard', workers=4)\n",
            "Start to train stage: GMM, named: gmm_train_new!\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3826: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "step:       20, time: 0.608, loss: 0.076038\n",
            "step:       40, time: 0.579, loss: 0.174177\n",
            "step:       60, time: 0.578, loss: 0.086810\n",
            "step:       80, time: 0.728, loss: 0.156248\n",
            "step:      100, time: 0.661, loss: 0.158212\n",
            "step:      120, time: 0.668, loss: 0.132279\n",
            "step:      140, time: 0.632, loss: 0.146852\n",
            "step:      160, time: 0.614, loss: 0.098327\n",
            "step:      180, time: 0.602, loss: 0.053721\n",
            "step:      200, time: 0.629, loss: 0.108041\n",
            "step:      220, time: 0.622, loss: 0.118760\n",
            "step:      240, time: 0.619, loss: 0.147240\n",
            "step:      260, time: 0.581, loss: 0.104840\n",
            "step:      280, time: 0.692, loss: 0.114260\n",
            "step:      300, time: 0.599, loss: 0.134067\n",
            "step:      320, time: 0.610, loss: 0.236127\n",
            "step:      340, time: 0.556, loss: 0.087651\n",
            "step:      360, time: 0.649, loss: 0.186804\n",
            "step:      380, time: 0.682, loss: 0.198278\n",
            "step:      400, time: 0.644, loss: 0.103931\n",
            "step:      420, time: 0.591, loss: 0.128980\n",
            "step:      440, time: 0.607, loss: 0.147639\n",
            "step:      460, time: 0.626, loss: 0.142624\n",
            "step:      480, time: 0.576, loss: 0.079114\n",
            "step:      500, time: 0.547, loss: 0.101477\n",
            "step:      520, time: 0.567, loss: 0.113824\n",
            "step:      540, time: 0.595, loss: 0.135152\n",
            "step:      560, time: 0.636, loss: 0.121002\n",
            "step:      580, time: 0.561, loss: 0.168318\n",
            "step:      600, time: 0.553, loss: 0.069844\n",
            "step:      620, time: 0.604, loss: 0.065273\n",
            "step:      640, time: 0.643, loss: 0.063236\n",
            "step:      660, time: 0.676, loss: 0.121830\n",
            "step:      680, time: 0.687, loss: 0.140832\n",
            "step:      700, time: 0.670, loss: 0.126243\n",
            "step:      720, time: 0.681, loss: 0.085236\n",
            "step:      740, time: 0.561, loss: 0.085914\n",
            "step:      760, time: 0.648, loss: 0.098715\n",
            "step:      780, time: 0.655, loss: 0.151142\n",
            "step:      800, time: 0.640, loss: 0.185962\n",
            "step:      820, time: 0.642, loss: 0.130808\n",
            "step:      840, time: 0.604, loss: 0.102189\n",
            "step:      860, time: 0.652, loss: 0.094681\n",
            "step:      880, time: 0.649, loss: 0.108581\n",
            "step:      900, time: 0.550, loss: 0.073362\n",
            "step:      920, time: 0.607, loss: 0.091619\n",
            "step:      940, time: 0.606, loss: 0.129636\n",
            "step:      960, time: 0.580, loss: 0.111350\n",
            "step:      980, time: 0.695, loss: 0.125975\n",
            "step:     1000, time: 0.600, loss: 0.139405\n",
            "step:     1020, time: 0.642, loss: 0.110915\n",
            "step:     1040, time: 0.573, loss: 0.112727\n",
            "step:     1060, time: 0.607, loss: 0.097495\n",
            "step:     1080, time: 0.577, loss: 0.152796\n",
            "step:     1100, time: 0.583, loss: 0.125268\n",
            "step:     1120, time: 0.606, loss: 0.081837\n",
            "step:     1140, time: 0.629, loss: 0.049094\n",
            "step:     1160, time: 0.568, loss: 0.076511\n",
            "step:     1180, time: 0.608, loss: 0.129258\n",
            "step:     1200, time: 0.622, loss: 0.083412\n",
            "step:     1220, time: 0.589, loss: 0.154493\n",
            "step:     1240, time: 0.506, loss: 0.181927\n",
            "step:     1260, time: 0.642, loss: 0.153368\n",
            "step:     1280, time: 0.590, loss: 0.106645\n",
            "step:     1300, time: 0.584, loss: 0.086467\n",
            "step:     1320, time: 0.564, loss: 0.055937\n",
            "step:     1340, time: 0.649, loss: 0.118773\n",
            "step:     1360, time: 0.624, loss: 0.153382\n",
            "step:     1380, time: 0.562, loss: 0.137039\n",
            "step:     1400, time: 0.640, loss: 0.133496\n",
            "step:     1420, time: 0.584, loss: 0.128049\n",
            "step:     1440, time: 0.658, loss: 0.108205\n",
            "step:     1460, time: 0.594, loss: 0.104630\n",
            "step:     1480, time: 0.544, loss: 0.077991\n",
            "step:     1500, time: 0.619, loss: 0.176063\n",
            "step:     1520, time: 0.627, loss: 0.068450\n",
            "step:     1540, time: 0.662, loss: 0.084466\n",
            "step:     1560, time: 0.571, loss: 0.038695\n",
            "step:     1580, time: 0.605, loss: 0.159443\n",
            "step:     1600, time: 0.585, loss: 0.076545\n",
            "step:     1620, time: 0.612, loss: 0.111610\n",
            "step:     1640, time: 0.692, loss: 0.100770\n",
            "step:     1660, time: 0.634, loss: 0.068712\n",
            "step:     1680, time: 0.587, loss: 0.122696\n",
            "step:     1700, time: 0.623, loss: 0.141680\n",
            "step:     1720, time: 0.575, loss: 0.062085\n",
            "step:     1740, time: 0.676, loss: 0.088950\n",
            "step:     1760, time: 0.588, loss: 0.121143\n",
            "step:     1780, time: 0.675, loss: 0.104471\n",
            "step:     1800, time: 0.709, loss: 0.114742\n",
            "step:     1820, time: 0.605, loss: 0.058724\n",
            "step:     1840, time: 0.643, loss: 0.239463\n",
            "step:     1860, time: 0.554, loss: 0.124350\n",
            "step:     1880, time: 0.616, loss: 0.126831\n",
            "step:     1900, time: 0.614, loss: 0.216575\n",
            "step:     1920, time: 0.622, loss: 0.123422\n",
            "step:     1940, time: 0.623, loss: 0.127953\n",
            "step:     1960, time: 0.616, loss: 0.131223\n",
            "step:     1980, time: 0.598, loss: 0.112329\n",
            "step:     2000, time: 0.560, loss: 0.098147\n",
            "step:     2020, time: 0.636, loss: 0.169877\n",
            "step:     2040, time: 0.602, loss: 0.093521\n",
            "step:     2060, time: 0.572, loss: 0.112858\n",
            "step:     2080, time: 0.595, loss: 0.074114\n",
            "step:     2100, time: 0.607, loss: 0.072686\n",
            "step:     2120, time: 0.620, loss: 0.136345\n",
            "step:     2140, time: 0.579, loss: 0.131077\n",
            "step:     2160, time: 0.581, loss: 0.110422\n",
            "step:     2180, time: 0.643, loss: 0.097661\n",
            "step:     2200, time: 0.714, loss: 0.092271\n",
            "step:     2220, time: 0.572, loss: 0.088867\n",
            "step:     2240, time: 0.601, loss: 0.062187\n",
            "step:     2260, time: 0.564, loss: 0.094269\n",
            "step:     2280, time: 0.593, loss: 0.261275\n",
            "step:     2300, time: 0.599, loss: 0.176647\n",
            "step:     2320, time: 0.568, loss: 0.064690\n",
            "step:     2340, time: 0.595, loss: 0.068739\n",
            "step:     2360, time: 0.657, loss: 0.149358\n",
            "step:     2380, time: 0.564, loss: 0.090374\n",
            "step:     2400, time: 0.608, loss: 0.075711\n",
            "step:     2420, time: 0.572, loss: 0.147777\n",
            "step:     2440, time: 0.633, loss: 0.084916\n",
            "step:     2460, time: 0.609, loss: 0.104231\n",
            "step:     2480, time: 0.575, loss: 0.092762\n",
            "step:     2500, time: 0.593, loss: 0.077524\n",
            "step:     2520, time: 0.663, loss: 0.281557\n",
            "step:     2540, time: 0.615, loss: 0.099907\n",
            "step:     2560, time: 0.564, loss: 0.078398\n",
            "step:     2580, time: 0.672, loss: 0.118369\n",
            "step:     2600, time: 0.609, loss: 0.138528\n",
            "step:     2620, time: 0.581, loss: 0.059313\n",
            "step:     2640, time: 0.630, loss: 0.107580\n",
            "step:     2660, time: 0.599, loss: 0.099931\n",
            "step:     2680, time: 0.619, loss: 0.143024\n",
            "step:     2700, time: 0.672, loss: 0.093026\n",
            "step:     2720, time: 0.561, loss: 0.075680\n",
            "step:     2740, time: 0.715, loss: 0.117356\n",
            "step:     2760, time: 0.589, loss: 0.108879\n",
            "step:     2780, time: 0.636, loss: 0.120011\n",
            "step:     2800, time: 0.550, loss: 0.091910\n",
            "step:     2820, time: 0.571, loss: 0.070641\n",
            "step:     2840, time: 0.617, loss: 0.093904\n",
            "step:     2860, time: 0.590, loss: 0.112236\n",
            "step:     2880, time: 0.651, loss: 0.096632\n",
            "step:     2900, time: 0.618, loss: 0.076449\n",
            "step:     2920, time: 0.566, loss: 0.160319\n",
            "step:     2940, time: 0.549, loss: 0.139218\n",
            "step:     2960, time: 0.583, loss: 0.088811\n",
            "step:     2980, time: 0.608, loss: 0.110841\n",
            "step:     3000, time: 0.584, loss: 0.087213\n",
            "step:     3020, time: 0.557, loss: 0.072797\n",
            "step:     3040, time: 0.606, loss: 0.101539\n",
            "step:     3060, time: 0.615, loss: 0.126335\n",
            "step:     3080, time: 0.588, loss: 0.146767\n",
            "step:     3100, time: 0.592, loss: 0.074324\n",
            "step:     3120, time: 0.588, loss: 0.124024\n",
            "step:     3140, time: 0.567, loss: 0.088082\n",
            "step:     3160, time: 0.572, loss: 0.102814\n",
            "step:     3180, time: 0.562, loss: 0.119214\n",
            "step:     3200, time: 0.650, loss: 0.128492\n",
            "step:     3220, time: 0.545, loss: 0.095789\n",
            "step:     3240, time: 0.688, loss: 0.154693\n",
            "step:     3260, time: 0.597, loss: 0.100397\n",
            "step:     3280, time: 0.621, loss: 0.079750\n",
            "step:     3300, time: 0.684, loss: 0.119331\n",
            "step:     3320, time: 0.611, loss: 0.137230\n",
            "step:     3340, time: 0.603, loss: 0.069948\n",
            "step:     3360, time: 0.576, loss: 0.062897\n",
            "step:     3380, time: 0.655, loss: 0.126799\n",
            "step:     3400, time: 0.640, loss: 0.089570\n",
            "step:     3420, time: 0.634, loss: 0.206792\n",
            "step:     3440, time: 0.600, loss: 0.117028\n",
            "step:     3460, time: 0.611, loss: 0.083708\n",
            "step:     3480, time: 0.543, loss: 0.158219\n",
            "step:     3500, time: 0.493, loss: 0.124209\n",
            "step:     3520, time: 0.676, loss: 0.173244\n",
            "step:     3540, time: 0.572, loss: 0.078655\n",
            "step:     3560, time: 0.693, loss: 0.181347\n",
            "step:     3580, time: 0.577, loss: 0.083367\n",
            "step:     3600, time: 0.608, loss: 0.106915\n",
            "step:     3620, time: 0.622, loss: 0.068995\n",
            "step:     3640, time: 0.513, loss: 0.087397\n",
            "step:     3660, time: 0.617, loss: 0.167466\n",
            "step:     3680, time: 0.606, loss: 0.087206\n",
            "step:     3700, time: 0.636, loss: 0.110646\n",
            "step:     3720, time: 0.609, loss: 0.108699\n",
            "step:     3740, time: 0.552, loss: 0.075476\n",
            "step:     3760, time: 0.595, loss: 0.128715\n",
            "step:     3780, time: 0.589, loss: 0.092004\n",
            "step:     3800, time: 0.623, loss: 0.094843\n",
            "step:     3820, time: 0.607, loss: 0.106452\n",
            "step:     3840, time: 0.569, loss: 0.105161\n",
            "step:     3860, time: 0.607, loss: 0.128874\n",
            "step:     3880, time: 0.579, loss: 0.090016\n",
            "step:     3900, time: 0.591, loss: 0.093776\n",
            "step:     3920, time: 0.534, loss: 0.081761\n",
            "step:     3940, time: 0.662, loss: 0.075957\n",
            "step:     3960, time: 0.527, loss: 0.069734\n",
            "step:     3980, time: 0.579, loss: 0.092794\n",
            "step:     4000, time: 0.586, loss: 0.104531\n",
            "step:     4020, time: 0.557, loss: 0.076564\n",
            "step:     4040, time: 0.617, loss: 0.083693\n",
            "step:     4060, time: 0.637, loss: 0.123799\n",
            "step:     4080, time: 0.559, loss: 0.112192\n",
            "step:     4100, time: 0.640, loss: 0.067340\n",
            "step:     4120, time: 0.631, loss: 0.097202\n",
            "step:     4140, time: 0.711, loss: 0.123849\n",
            "step:     4160, time: 0.621, loss: 0.231747\n",
            "step:     4180, time: 0.670, loss: 0.079210\n",
            "step:     4200, time: 0.656, loss: 0.069305\n",
            "step:     4220, time: 0.598, loss: 0.118327\n",
            "step:     4240, time: 0.603, loss: 0.124271\n",
            "step:     4260, time: 0.608, loss: 0.080415\n",
            "step:     4280, time: 0.609, loss: 0.065394\n",
            "step:     4300, time: 0.570, loss: 0.062727\n",
            "step:     4320, time: 0.665, loss: 0.068947\n",
            "step:     4340, time: 0.640, loss: 0.137200\n",
            "step:     4360, time: 0.645, loss: 0.060076\n",
            "step:     4380, time: 0.565, loss: 0.096535\n",
            "step:     4400, time: 0.627, loss: 0.139462\n",
            "step:     4420, time: 0.606, loss: 0.069732\n",
            "step:     4440, time: 0.564, loss: 0.092605\n",
            "step:     4460, time: 0.604, loss: 0.090389\n",
            "step:     4480, time: 0.618, loss: 0.170773\n",
            "step:     4500, time: 0.576, loss: 0.075337\n",
            "step:     4520, time: 0.625, loss: 0.157822\n",
            "step:     4540, time: 0.634, loss: 0.110895\n",
            "step:     4560, time: 0.645, loss: 0.177472\n",
            "step:     4580, time: 0.564, loss: 0.073394\n",
            "step:     4600, time: 0.595, loss: 0.087848\n",
            "step:     4620, time: 0.614, loss: 0.113988\n",
            "step:     4640, time: 0.590, loss: 0.109485\n",
            "step:     4660, time: 0.575, loss: 0.121046\n",
            "step:     4680, time: 0.672, loss: 0.091678\n",
            "step:     4700, time: 0.623, loss: 0.120950\n",
            "step:     4720, time: 0.599, loss: 0.115212\n",
            "step:     4740, time: 0.594, loss: 0.092843\n",
            "step:     4760, time: 0.611, loss: 0.114516\n",
            "step:     4780, time: 0.587, loss: 0.075708\n",
            "step:     4800, time: 0.594, loss: 0.100598\n",
            "step:     4820, time: 0.623, loss: 0.123149\n",
            "step:     4840, time: 0.631, loss: 0.100982\n",
            "step:     4860, time: 0.621, loss: 0.062567\n",
            "step:     4880, time: 0.618, loss: 0.091983\n",
            "step:     4900, time: 0.611, loss: 0.155478\n",
            "step:     4920, time: 0.603, loss: 0.143546\n",
            "step:     4940, time: 0.617, loss: 0.077756\n",
            "step:     4960, time: 0.699, loss: 0.091094\n",
            "step:     4980, time: 0.604, loss: 0.083642\n",
            "step:     5000, time: 0.612, loss: 0.110401\n",
            "step:     5020, time: 0.564, loss: 0.136788\n",
            "step:     5040, time: 0.603, loss: 0.085355\n",
            "step:     5060, time: 0.597, loss: 0.105273\n",
            "step:     5080, time: 0.653, loss: 0.124732\n",
            "step:     5100, time: 0.564, loss: 0.096806\n",
            "step:     5120, time: 0.577, loss: 0.048575\n",
            "step:     5140, time: 0.577, loss: 0.061327\n",
            "step:     5160, time: 0.599, loss: 0.144667\n",
            "step:     5180, time: 0.644, loss: 0.149798\n",
            "step:     5200, time: 0.580, loss: 0.088900\n",
            "step:     5220, time: 0.641, loss: 0.066809\n",
            "step:     5240, time: 0.584, loss: 0.057015\n",
            "step:     5260, time: 0.621, loss: 0.094206\n",
            "step:     5280, time: 0.595, loss: 0.086662\n",
            "step:     5300, time: 0.569, loss: 0.061961\n",
            "step:     5320, time: 0.596, loss: 0.097542\n",
            "step:     5340, time: 0.563, loss: 0.094400\n",
            "step:     5360, time: 0.658, loss: 0.093974\n",
            "step:     5380, time: 0.656, loss: 0.131508\n",
            "step:     5400, time: 0.606, loss: 0.104483\n",
            "step:     5420, time: 0.604, loss: 0.089551\n",
            "step:     5440, time: 0.548, loss: 0.092826\n",
            "step:     5460, time: 0.641, loss: 0.090420\n",
            "step:     5480, time: 0.550, loss: 0.068213\n",
            "step:     5500, time: 0.572, loss: 0.074776\n",
            "step:     5520, time: 0.625, loss: 0.102403\n",
            "step:     5540, time: 0.542, loss: 0.087980\n",
            "step:     5560, time: 0.649, loss: 0.118660\n",
            "step:     5580, time: 0.569, loss: 0.108490\n",
            "step:     5600, time: 0.611, loss: 0.085490\n",
            "step:     5620, time: 0.596, loss: 0.198017\n",
            "step:     5640, time: 0.566, loss: 0.109385\n",
            "step:     5660, time: 0.649, loss: 0.182793\n",
            "step:     5680, time: 0.703, loss: 0.161536\n",
            "step:     5700, time: 0.596, loss: 0.062058\n",
            "step:     5720, time: 0.662, loss: 0.113313\n",
            "step:     5740, time: 0.560, loss: 0.075641\n",
            "step:     5760, time: 0.577, loss: 0.087457\n",
            "step:     5780, time: 0.620, loss: 0.108664\n",
            "step:     5800, time: 0.642, loss: 0.107574\n",
            "step:     5820, time: 0.614, loss: 0.072963\n",
            "step:     5840, time: 0.672, loss: 0.102756\n",
            "step:     5860, time: 0.606, loss: 0.097710\n",
            "step:     5880, time: 0.626, loss: 0.071795\n",
            "step:     5900, time: 0.618, loss: 0.081987\n",
            "step:     5920, time: 0.580, loss: 0.128279\n",
            "step:     5940, time: 0.596, loss: 0.117876\n",
            "step:     5960, time: 0.614, loss: 0.075307\n",
            "step:     5980, time: 0.594, loss: 0.079513\n",
            "step:     6000, time: 0.633, loss: 0.114692\n",
            "step:     6020, time: 0.568, loss: 0.133627\n",
            "step:     6040, time: 0.638, loss: 0.147060\n",
            "step:     6060, time: 0.584, loss: 0.118504\n",
            "step:     6080, time: 0.554, loss: 0.129379\n",
            "step:     6100, time: 0.635, loss: 0.094543\n",
            "step:     6120, time: 0.551, loss: 0.104092\n",
            "step:     6140, time: 0.619, loss: 0.115607\n",
            "step:     6160, time: 0.593, loss: 0.101980\n",
            "step:     6180, time: 0.613, loss: 0.114507\n",
            "step:     6200, time: 0.616, loss: 0.081464\n",
            "step:     6220, time: 0.662, loss: 0.084683\n",
            "step:     6240, time: 0.648, loss: 0.106942\n",
            "step:     6260, time: 0.556, loss: 0.092090\n",
            "step:     6280, time: 0.622, loss: 0.127494\n",
            "step:     6300, time: 0.584, loss: 0.070667\n",
            "step:     6320, time: 0.584, loss: 0.260092\n",
            "step:     6340, time: 0.647, loss: 0.043139\n",
            "step:     6360, time: 0.567, loss: 0.120886\n",
            "step:     6380, time: 0.619, loss: 0.110880\n",
            "step:     6400, time: 0.614, loss: 0.096528\n",
            "step:     6420, time: 0.587, loss: 0.123296\n",
            "step:     6440, time: 0.611, loss: 0.129494\n",
            "step:     6460, time: 0.596, loss: 0.151033\n",
            "step:     6480, time: 0.673, loss: 0.066910\n",
            "step:     6500, time: 0.633, loss: 0.102952\n",
            "step:     6520, time: 0.596, loss: 0.092716\n",
            "step:     6540, time: 0.559, loss: 0.127128\n",
            "step:     6560, time: 0.668, loss: 0.100888\n",
            "step:     6580, time: 0.639, loss: 0.102995\n",
            "step:     6600, time: 0.629, loss: 0.057951\n",
            "step:     6620, time: 0.573, loss: 0.124062\n",
            "step:     6640, time: 0.588, loss: 0.097608\n",
            "step:     6660, time: 0.612, loss: 0.075660\n",
            "step:     6680, time: 0.591, loss: 0.058744\n",
            "step:     6700, time: 0.597, loss: 0.055923\n",
            "step:     6720, time: 0.595, loss: 0.104667\n",
            "step:     6740, time: 0.630, loss: 0.122465\n",
            "step:     6760, time: 0.654, loss: 0.043603\n",
            "step:     6780, time: 0.621, loss: 0.156751\n",
            "step:     6800, time: 0.621, loss: 0.094174\n",
            "step:     6820, time: 0.620, loss: 0.135303\n",
            "step:     6840, time: 0.565, loss: 0.069793\n",
            "step:     6860, time: 0.540, loss: 0.099020\n",
            "step:     6880, time: 0.619, loss: 0.111202\n",
            "step:     6900, time: 0.611, loss: 0.104131\n",
            "step:     6920, time: 0.583, loss: 0.105322\n",
            "step:     6940, time: 0.580, loss: 0.092128\n",
            "step:     6960, time: 0.629, loss: 0.069518\n",
            "step:     6980, time: 0.715, loss: 0.084025\n",
            "step:     7000, time: 0.580, loss: 0.105253\n",
            "step:     7020, time: 0.661, loss: 0.116954\n",
            "step:     7040, time: 0.554, loss: 0.106383\n",
            "step:     7060, time: 0.588, loss: 0.097520\n",
            "step:     7080, time: 0.685, loss: 0.083189\n",
            "step:     7100, time: 0.598, loss: 0.089106\n",
            "step:     7120, time: 0.596, loss: 0.083096\n",
            "step:     7140, time: 0.618, loss: 0.134679\n",
            "step:     7160, time: 0.592, loss: 0.094521\n",
            "step:     7180, time: 0.546, loss: 0.086386\n",
            "step:     7200, time: 0.665, loss: 0.116486\n",
            "step:     7220, time: 0.605, loss: 0.096163\n",
            "step:     7240, time: 0.582, loss: 0.114760\n",
            "step:     7260, time: 0.564, loss: 0.071134\n",
            "step:     7280, time: 0.640, loss: 0.139615\n",
            "step:     7300, time: 0.585, loss: 0.153715\n",
            "step:     7320, time: 0.567, loss: 0.044229\n",
            "step:     7340, time: 0.656, loss: 0.105044\n",
            "step:     7360, time: 0.576, loss: 0.063087\n",
            "step:     7380, time: 0.601, loss: 0.072552\n",
            "step:     7400, time: 0.599, loss: 0.073876\n",
            "step:     7420, time: 0.605, loss: 0.076561\n",
            "step:     7440, time: 0.630, loss: 0.102483\n",
            "step:     7460, time: 0.626, loss: 0.112078\n",
            "step:     7480, time: 0.649, loss: 0.196947\n",
            "step:     7500, time: 0.589, loss: 0.070223\n",
            "step:     7520, time: 0.579, loss: 0.072421\n",
            "step:     7540, time: 0.606, loss: 0.094888\n",
            "step:     7560, time: 0.578, loss: 0.093170\n",
            "step:     7580, time: 0.567, loss: 0.066248\n",
            "step:     7600, time: 0.604, loss: 0.068106\n",
            "step:     7620, time: 0.523, loss: 0.099497\n",
            "step:     7640, time: 0.548, loss: 0.054540\n",
            "step:     7660, time: 0.597, loss: 0.096404\n",
            "step:     7680, time: 0.607, loss: 0.123034\n",
            "step:     7700, time: 0.626, loss: 0.117279\n",
            "step:     7720, time: 0.544, loss: 0.144366\n",
            "step:     7740, time: 0.589, loss: 0.092210\n",
            "step:     7760, time: 0.585, loss: 0.075840\n",
            "step:     7780, time: 0.550, loss: 0.057181\n",
            "step:     7800, time: 0.649, loss: 0.124625\n",
            "step:     7820, time: 0.563, loss: 0.089493\n",
            "step:     7840, time: 0.593, loss: 0.106134\n",
            "step:     7860, time: 0.648, loss: 0.086656\n",
            "step:     7880, time: 0.559, loss: 0.105599\n",
            "step:     7900, time: 0.575, loss: 0.076056\n",
            "step:     7920, time: 0.568, loss: 0.100345\n",
            "step:     7940, time: 0.586, loss: 0.056899\n",
            "step:     7960, time: 0.596, loss: 0.105844\n",
            "step:     7980, time: 0.639, loss: 0.093064\n",
            "step:     8000, time: 0.597, loss: 0.113182\n",
            "step:     8020, time: 0.530, loss: 0.078337\n",
            "step:     8040, time: 0.577, loss: 0.113290\n",
            "step:     8060, time: 0.596, loss: 0.110689\n",
            "step:     8080, time: 0.599, loss: 0.141340\n",
            "step:     8100, time: 0.626, loss: 0.118379\n",
            "step:     8120, time: 0.569, loss: 0.046713\n",
            "step:     8140, time: 0.618, loss: 0.094017\n",
            "step:     8160, time: 0.645, loss: 0.146153\n",
            "step:     8180, time: 0.609, loss: 0.103852\n",
            "step:     8200, time: 0.599, loss: 0.047330\n",
            "step:     8220, time: 0.611, loss: 0.082437\n",
            "step:     8240, time: 0.616, loss: 0.130830\n",
            "step:     8260, time: 0.600, loss: 0.062071\n",
            "step:     8280, time: 0.554, loss: 0.051205\n",
            "step:     8300, time: 0.645, loss: 0.094077\n",
            "step:     8320, time: 0.557, loss: 0.106291\n",
            "step:     8340, time: 0.575, loss: 0.110671\n",
            "step:     8360, time: 0.617, loss: 0.139111\n",
            "step:     8380, time: 0.592, loss: 0.067943\n",
            "step:     8400, time: 0.582, loss: 0.080326\n",
            "step:     8420, time: 0.543, loss: 0.077340\n",
            "step:     8440, time: 0.580, loss: 0.046847\n",
            "step:     8460, time: 0.623, loss: 0.135118\n",
            "step:     8480, time: 0.612, loss: 0.087673\n",
            "step:     8500, time: 0.550, loss: 0.057214\n",
            "step:     8520, time: 0.559, loss: 0.062237\n",
            "step:     8540, time: 0.578, loss: 0.070035\n",
            "step:     8560, time: 0.584, loss: 0.098158\n",
            "step:     8580, time: 0.569, loss: 0.080066\n",
            "step:     8600, time: 0.644, loss: 0.144663\n",
            "step:     8620, time: 0.574, loss: 0.079870\n",
            "step:     8640, time: 0.605, loss: 0.067397\n",
            "step:     8660, time: 0.607, loss: 0.079564\n",
            "step:     8680, time: 0.633, loss: 0.066532\n",
            "step:     8700, time: 0.557, loss: 0.068237\n",
            "step:     8720, time: 0.562, loss: 0.063407\n",
            "step:     8740, time: 0.596, loss: 0.075315\n",
            "step:     8760, time: 0.611, loss: 0.115658\n",
            "step:     8780, time: 0.641, loss: 0.119682\n",
            "step:     8800, time: 0.586, loss: 0.070899\n",
            "step:     8820, time: 0.682, loss: 0.160832\n",
            "step:     8840, time: 0.554, loss: 0.090391\n",
            "step:     8860, time: 0.566, loss: 0.095466\n",
            "step:     8880, time: 0.599, loss: 0.108614\n",
            "step:     8900, time: 0.657, loss: 0.141541\n",
            "step:     8920, time: 0.631, loss: 0.097081\n",
            "step:     8940, time: 0.615, loss: 0.087497\n",
            "step:     8960, time: 0.611, loss: 0.096897\n",
            "step:     8980, time: 0.605, loss: 0.098912\n",
            "step:     9000, time: 0.550, loss: 0.093315\n",
            "step:     9020, time: 0.584, loss: 0.115048\n",
            "step:     9040, time: 0.587, loss: 0.093588\n",
            "step:     9060, time: 0.559, loss: 0.060439\n",
            "step:     9080, time: 0.603, loss: 0.073041\n",
            "step:     9100, time: 0.548, loss: 0.084473\n",
            "step:     9120, time: 0.577, loss: 0.048473\n",
            "step:     9140, time: 0.595, loss: 0.203113\n",
            "step:     9160, time: 0.532, loss: 0.071001\n",
            "step:     9180, time: 0.618, loss: 0.078978\n",
            "step:     9200, time: 0.593, loss: 0.112904\n",
            "step:     9220, time: 0.630, loss: 0.187814\n",
            "step:     9240, time: 0.548, loss: 0.059383\n",
            "step:     9260, time: 0.600, loss: 0.110708\n",
            "step:     9280, time: 0.631, loss: 0.073317\n",
            "step:     9300, time: 0.577, loss: 0.122373\n",
            "step:     9320, time: 0.589, loss: 0.120727\n",
            "step:     9340, time: 0.609, loss: 0.093742\n",
            "step:     9360, time: 0.588, loss: 0.076138\n",
            "step:     9380, time: 0.683, loss: 0.127321\n",
            "step:     9400, time: 0.608, loss: 0.104904\n",
            "step:     9420, time: 0.518, loss: 0.059040\n",
            "step:     9440, time: 0.625, loss: 0.128978\n",
            "step:     9460, time: 0.605, loss: 0.118348\n",
            "step:     9480, time: 0.600, loss: 0.057189\n",
            "step:     9500, time: 0.611, loss: 0.099493\n",
            "step:     9520, time: 0.621, loss: 0.110090\n",
            "step:     9540, time: 0.607, loss: 0.141030\n",
            "step:     9560, time: 0.673, loss: 0.090821\n",
            "step:     9580, time: 0.635, loss: 0.121612\n",
            "step:     9600, time: 0.588, loss: 0.090825\n",
            "step:     9620, time: 0.635, loss: 0.080478\n",
            "step:     9640, time: 0.565, loss: 0.033445\n",
            "step:     9660, time: 0.596, loss: 0.150621\n",
            "step:     9680, time: 0.555, loss: 0.137022\n",
            "step:     9700, time: 0.569, loss: 0.096497\n",
            "step:     9720, time: 0.579, loss: 0.086134\n",
            "step:     9740, time: 0.572, loss: 0.043564\n",
            "step:     9760, time: 0.559, loss: 0.080444\n",
            "step:     9780, time: 0.617, loss: 0.145569\n",
            "step:     9800, time: 0.600, loss: 0.077059\n",
            "step:     9820, time: 0.652, loss: 0.105195\n",
            "step:     9840, time: 0.595, loss: 0.068541\n",
            "step:     9860, time: 0.524, loss: 0.063875\n",
            "step:     9880, time: 0.579, loss: 0.097691\n",
            "step:     9900, time: 0.641, loss: 0.117452\n",
            "step:     9920, time: 0.566, loss: 0.070628\n",
            "step:     9940, time: 0.536, loss: 0.071096\n",
            "step:     9960, time: 0.588, loss: 0.094206\n",
            "step:     9980, time: 0.630, loss: 0.103229\n",
            "step:    10000, time: 0.588, loss: 0.074630\n",
            "step:    10020, time: 0.535, loss: 0.089370\n",
            "step:    10040, time: 0.582, loss: 0.129188\n",
            "step:    10060, time: 0.588, loss: 0.041457\n",
            "step:    10080, time: 0.596, loss: 0.086282\n",
            "step:    10100, time: 0.551, loss: 0.066962\n",
            "step:    10120, time: 0.619, loss: 0.122424\n",
            "step:    10140, time: 0.594, loss: 0.053947\n",
            "step:    10160, time: 0.594, loss: 0.071305\n",
            "step:    10180, time: 0.597, loss: 0.086735\n",
            "step:    10200, time: 0.607, loss: 0.106270\n",
            "step:    10220, time: 0.580, loss: 0.076010\n",
            "step:    10240, time: 0.617, loss: 0.058532\n",
            "step:    10260, time: 0.575, loss: 0.085000\n",
            "step:    10280, time: 0.594, loss: 0.076551\n",
            "step:    10300, time: 0.629, loss: 0.122044\n",
            "step:    10320, time: 0.612, loss: 0.106880\n",
            "step:    10340, time: 0.594, loss: 0.130323\n",
            "step:    10360, time: 0.597, loss: 0.154000\n",
            "step:    10380, time: 0.622, loss: 0.071711\n",
            "step:    10400, time: 0.554, loss: 0.084535\n",
            "step:    10420, time: 0.576, loss: 0.087363\n",
            "step:    10440, time: 0.612, loss: 0.093138\n",
            "step:    10460, time: 0.610, loss: 0.064324\n",
            "step:    10480, time: 0.589, loss: 0.061873\n",
            "step:    10500, time: 0.564, loss: 0.110942\n",
            "step:    10520, time: 0.572, loss: 0.050025\n",
            "step:    10540, time: 0.604, loss: 0.051525\n",
            "step:    10560, time: 0.601, loss: 0.076305\n",
            "step:    10580, time: 0.593, loss: 0.066960\n",
            "step:    10600, time: 0.624, loss: 0.076521\n",
            "step:    10620, time: 0.612, loss: 0.094082\n",
            "step:    10640, time: 0.598, loss: 0.054849\n",
            "step:    10660, time: 0.505, loss: 0.079380\n",
            "step:    10680, time: 0.707, loss: 0.084510\n",
            "step:    10700, time: 0.552, loss: 0.085732\n",
            "step:    10720, time: 0.587, loss: 0.107476\n",
            "step:    10740, time: 0.605, loss: 0.130173\n",
            "step:    10760, time: 0.567, loss: 0.062359\n",
            "step:    10780, time: 0.572, loss: 0.054751\n",
            "step:    10800, time: 0.562, loss: 0.112471\n",
            "step:    10820, time: 0.558, loss: 0.125083\n",
            "step:    10840, time: 0.600, loss: 0.085349\n",
            "step:    10860, time: 0.631, loss: 0.122488\n",
            "step:    10880, time: 0.640, loss: 0.081272\n",
            "step:    10900, time: 0.607, loss: 0.053585\n",
            "step:    10920, time: 0.616, loss: 0.083675\n",
            "step:    10940, time: 0.573, loss: 0.168788\n",
            "step:    10960, time: 0.624, loss: 0.068696\n",
            "step:    10980, time: 0.680, loss: 0.116835\n",
            "step:    11000, time: 0.579, loss: 0.138941\n",
            "step:    11020, time: 0.619, loss: 0.121079\n",
            "step:    11040, time: 0.633, loss: 0.085167\n",
            "step:    11060, time: 0.586, loss: 0.113649\n",
            "step:    11080, time: 0.578, loss: 0.067576\n",
            "step:    11100, time: 0.551, loss: 0.047255\n",
            "step:    11120, time: 0.603, loss: 0.047487\n",
            "step:    11140, time: 0.707, loss: 0.118235\n",
            "step:    11160, time: 0.648, loss: 0.102524\n",
            "step:    11180, time: 0.567, loss: 0.066837\n",
            "step:    11200, time: 0.624, loss: 0.063947\n",
            "step:    11220, time: 0.618, loss: 0.135324\n",
            "step:    11240, time: 0.555, loss: 0.138968\n",
            "step:    11260, time: 0.535, loss: 0.088398\n",
            "step:    11280, time: 0.599, loss: 0.067696\n",
            "step:    11300, time: 0.622, loss: 0.064046\n",
            "step:    11320, time: 0.603, loss: 0.093133\n",
            "step:    11340, time: 0.570, loss: 0.068633\n",
            "step:    11360, time: 0.572, loss: 0.059673\n",
            "step:    11380, time: 0.641, loss: 0.079358\n",
            "step:    11400, time: 0.588, loss: 0.046740\n",
            "step:    11420, time: 0.602, loss: 0.159024\n",
            "step:    11440, time: 0.621, loss: 0.153081\n",
            "step:    11460, time: 0.610, loss: 0.053772\n",
            "step:    11480, time: 0.562, loss: 0.045636\n",
            "step:    11500, time: 0.587, loss: 0.078017\n",
            "step:    11520, time: 0.609, loss: 0.069529\n",
            "step:    11540, time: 0.648, loss: 0.092669\n",
            "step:    11560, time: 0.571, loss: 0.124337\n",
            "step:    11580, time: 0.577, loss: 0.055296\n",
            "step:    11600, time: 0.527, loss: 0.085130\n",
            "step:    11620, time: 0.570, loss: 0.099501\n",
            "step:    11640, time: 0.617, loss: 0.100745\n",
            "step:    11660, time: 0.614, loss: 0.058311\n",
            "step:    11680, time: 0.590, loss: 0.080087\n",
            "step:    11700, time: 0.607, loss: 0.105042\n",
            "step:    11720, time: 0.592, loss: 0.096959\n",
            "step:    11740, time: 0.594, loss: 0.109940\n",
            "step:    11760, time: 0.563, loss: 0.073440\n",
            "step:    11780, time: 0.583, loss: 0.130292\n",
            "step:    11800, time: 0.558, loss: 0.052438\n",
            "step:    11820, time: 0.597, loss: 0.088674\n",
            "step:    11840, time: 0.585, loss: 0.094544\n",
            "step:    11860, time: 0.625, loss: 0.052052\n",
            "step:    11880, time: 0.545, loss: 0.096596\n",
            "step:    11900, time: 0.678, loss: 0.105181\n",
            "step:    11920, time: 0.629, loss: 0.065991\n",
            "step:    11940, time: 0.566, loss: 0.039325\n",
            "step:    11960, time: 0.620, loss: 0.070109\n",
            "step:    11980, time: 0.573, loss: 0.097004\n",
            "step:    12000, time: 0.558, loss: 0.096555\n",
            "step:    12020, time: 0.608, loss: 0.111445\n",
            "step:    12040, time: 0.617, loss: 0.074792\n",
            "step:    12060, time: 0.552, loss: 0.070995\n",
            "step:    12080, time: 0.570, loss: 0.044260\n",
            "step:    12100, time: 0.578, loss: 0.112817\n",
            "step:    12120, time: 0.533, loss: 0.110365\n",
            "step:    12140, time: 0.585, loss: 0.074077\n",
            "step:    12160, time: 0.682, loss: 0.127932\n",
            "step:    12180, time: 0.613, loss: 0.094730\n",
            "step:    12200, time: 0.564, loss: 0.072661\n",
            "step:    12220, time: 0.534, loss: 0.082728\n",
            "step:    12240, time: 0.606, loss: 0.060579\n",
            "step:    12260, time: 0.563, loss: 0.098011\n",
            "step:    12280, time: 0.508, loss: 0.080259\n",
            "step:    12300, time: 0.629, loss: 0.076722\n",
            "step:    12320, time: 0.645, loss: 0.060327\n",
            "step:    12340, time: 0.590, loss: 0.076652\n",
            "step:    12360, time: 0.621, loss: 0.122149\n",
            "step:    12380, time: 0.523, loss: 0.094821\n",
            "step:    12400, time: 0.622, loss: 0.068995\n",
            "step:    12420, time: 0.629, loss: 0.134373\n",
            "step:    12440, time: 0.588, loss: 0.054732\n",
            "step:    12460, time: 0.556, loss: 0.092896\n",
            "step:    12480, time: 0.643, loss: 0.122852\n",
            "step:    12500, time: 0.572, loss: 0.081214\n",
            "step:    12520, time: 0.554, loss: 0.087393\n",
            "step:    12540, time: 0.649, loss: 0.076224\n",
            "step:    12560, time: 0.531, loss: 0.078989\n",
            "step:    12580, time: 0.593, loss: 0.124657\n",
            "step:    12600, time: 0.584, loss: 0.119464\n",
            "step:    12620, time: 0.568, loss: 0.081639\n",
            "step:    12640, time: 0.571, loss: 0.058219\n",
            "step:    12660, time: 0.595, loss: 0.074567\n",
            "step:    12680, time: 0.588, loss: 0.103845\n",
            "step:    12700, time: 0.590, loss: 0.109134\n",
            "step:    12720, time: 0.636, loss: 0.091947\n",
            "step:    12740, time: 0.614, loss: 0.071882\n",
            "step:    12760, time: 0.562, loss: 0.059551\n",
            "step:    12780, time: 0.642, loss: 0.125527\n",
            "step:    12800, time: 0.639, loss: 0.087239\n",
            "step:    12820, time: 0.587, loss: 0.079920\n",
            "step:    12840, time: 0.571, loss: 0.064944\n",
            "step:    12860, time: 0.646, loss: 0.301683\n",
            "step:    12880, time: 0.592, loss: 0.056181\n",
            "step:    12900, time: 0.561, loss: 0.070082\n",
            "step:    12920, time: 0.537, loss: 0.045853\n",
            "step:    12940, time: 0.600, loss: 0.074641\n",
            "step:    12960, time: 0.568, loss: 0.072700\n",
            "step:    12980, time: 0.572, loss: 0.085318\n",
            "step:    13000, time: 0.519, loss: 0.065540\n",
            "step:    13020, time: 0.542, loss: 0.090653\n",
            "step:    13040, time: 0.579, loss: 0.075599\n",
            "step:    13060, time: 0.617, loss: 0.090125\n",
            "step:    13080, time: 0.562, loss: 0.147726\n",
            "step:    13100, time: 0.595, loss: 0.081867\n",
            "step:    13120, time: 0.678, loss: 0.146173\n",
            "step:    13140, time: 0.650, loss: 0.073421\n",
            "step:    13160, time: 0.604, loss: 0.105205\n",
            "step:    13180, time: 0.621, loss: 0.080602\n",
            "step:    13200, time: 0.656, loss: 0.097620\n",
            "step:    13220, time: 0.578, loss: 0.083541\n",
            "step:    13240, time: 0.631, loss: 0.096677\n",
            "step:    13260, time: 0.607, loss: 0.120011\n",
            "step:    13280, time: 0.601, loss: 0.062295\n",
            "step:    13300, time: 0.552, loss: 0.073227\n",
            "step:    13320, time: 0.644, loss: 0.114902\n",
            "step:    13340, time: 0.578, loss: 0.139477\n",
            "step:    13360, time: 0.539, loss: 0.061147\n",
            "step:    13380, time: 0.578, loss: 0.084772\n",
            "step:    13400, time: 0.546, loss: 0.074606\n",
            "step:    13420, time: 0.638, loss: 0.098129\n",
            "step:    13440, time: 0.606, loss: 0.078602\n",
            "step:    13460, time: 0.699, loss: 0.092459\n",
            "step:    13480, time: 0.571, loss: 0.088115\n",
            "step:    13500, time: 0.553, loss: 0.061659\n",
            "step:    13520, time: 0.584, loss: 0.124131\n",
            "step:    13540, time: 0.591, loss: 0.075353\n",
            "step:    13560, time: 0.656, loss: 0.100751\n",
            "step:    13580, time: 0.579, loss: 0.041603\n",
            "step:    13600, time: 0.646, loss: 0.108821\n",
            "step:    13620, time: 0.623, loss: 0.111376\n",
            "step:    13640, time: 0.537, loss: 0.097061\n",
            "step:    13660, time: 0.613, loss: 0.104478\n",
            "step:    13680, time: 0.609, loss: 0.105952\n",
            "step:    13700, time: 0.602, loss: 0.068551\n",
            "step:    13720, time: 0.565, loss: 0.062857\n",
            "step:    13740, time: 0.585, loss: 0.089643\n",
            "step:    13760, time: 0.574, loss: 0.109471\n",
            "step:    13780, time: 0.656, loss: 0.100219\n",
            "step:    13800, time: 0.535, loss: 0.094374\n",
            "step:    13820, time: 0.629, loss: 0.064106\n",
            "step:    13840, time: 0.548, loss: 0.077693\n",
            "step:    13860, time: 0.612, loss: 0.115179\n",
            "step:    13880, time: 0.579, loss: 0.069608\n",
            "step:    13900, time: 0.561, loss: 0.060801\n",
            "step:    13920, time: 0.604, loss: 0.111769\n",
            "step:    13940, time: 0.617, loss: 0.063988\n",
            "step:    13960, time: 0.619, loss: 0.110482\n",
            "step:    13980, time: 0.562, loss: 0.088223\n",
            "step:    14000, time: 0.600, loss: 0.120326\n",
            "step:    14020, time: 0.583, loss: 0.073327\n",
            "step:    14040, time: 0.687, loss: 0.113107\n",
            "step:    14060, time: 0.536, loss: 0.059650\n",
            "step:    14080, time: 0.590, loss: 0.118854\n",
            "step:    14100, time: 0.623, loss: 0.078474\n",
            "step:    14120, time: 0.565, loss: 0.050918\n",
            "step:    14140, time: 0.652, loss: 0.080335\n",
            "step:    14160, time: 0.662, loss: 0.097436\n",
            "step:    14180, time: 0.640, loss: 0.122751\n",
            "step:    14200, time: 0.624, loss: 0.071578\n",
            "step:    14220, time: 0.452, loss: 0.071860\n",
            "step:    14240, time: 0.550, loss: 0.070118\n",
            "step:    14260, time: 0.606, loss: 0.075814\n",
            "step:    14280, time: 0.591, loss: 0.112603\n",
            "step:    14300, time: 0.572, loss: 0.233158\n",
            "step:    14320, time: 0.581, loss: 0.067179\n",
            "step:    14340, time: 0.552, loss: 0.074944\n",
            "step:    14360, time: 0.514, loss: 0.081799\n",
            "step:    14380, time: 0.622, loss: 0.107719\n",
            "step:    14400, time: 0.687, loss: 0.091418\n",
            "step:    14420, time: 0.720, loss: 0.107818\n",
            "step:    14440, time: 0.630, loss: 0.103100\n",
            "step:    14460, time: 0.598, loss: 0.065247\n",
            "step:    14480, time: 0.550, loss: 0.061461\n",
            "step:    14500, time: 0.627, loss: 0.140614\n",
            "step:    14520, time: 0.618, loss: 0.069752\n",
            "step:    14540, time: 0.579, loss: 0.116397\n",
            "step:    14560, time: 0.584, loss: 0.070214\n",
            "step:    14580, time: 0.629, loss: 0.064414\n",
            "step:    14600, time: 0.591, loss: 0.109499\n",
            "step:    14620, time: 0.580, loss: 0.076469\n",
            "step:    14640, time: 0.574, loss: 0.071018\n",
            "step:    14660, time: 0.607, loss: 0.079824\n",
            "step:    14680, time: 0.578, loss: 0.078585\n",
            "step:    14700, time: 0.533, loss: 0.053515\n",
            "step:    14720, time: 0.548, loss: 0.080104\n",
            "step:    14740, time: 0.607, loss: 0.075069\n",
            "step:    14760, time: 0.593, loss: 0.077433\n",
            "step:    14780, time: 0.599, loss: 0.090426\n",
            "step:    14800, time: 0.647, loss: 0.105957\n",
            "step:    14820, time: 0.554, loss: 0.072182\n",
            "step:    14840, time: 0.609, loss: 0.100647\n",
            "step:    14860, time: 0.613, loss: 0.087363\n",
            "step:    14880, time: 0.630, loss: 0.118873\n",
            "step:    14900, time: 0.604, loss: 0.085044\n",
            "step:    14920, time: 0.580, loss: 0.077815\n",
            "step:    14940, time: 0.624, loss: 0.067642\n",
            "step:    14960, time: 0.614, loss: 0.101921\n",
            "step:    14980, time: 0.616, loss: 0.125359\n",
            "step:    15000, time: 0.564, loss: 0.106761\n",
            "step:    15020, time: 0.623, loss: 0.084984\n",
            "step:    15040, time: 0.673, loss: 0.110846\n",
            "step:    15060, time: 0.652, loss: 0.143051\n",
            "step:    15080, time: 0.613, loss: 0.125310\n",
            "step:    15100, time: 0.579, loss: 0.099315\n",
            "step:    15120, time: 0.553, loss: 0.062296\n",
            "step:    15140, time: 0.683, loss: 0.107496\n",
            "step:    15160, time: 0.580, loss: 0.076019\n",
            "step:    15180, time: 0.584, loss: 0.058944\n",
            "step:    15200, time: 0.606, loss: 0.119908\n",
            "step:    15220, time: 0.637, loss: 0.097876\n",
            "step:    15240, time: 0.633, loss: 0.124019\n",
            "step:    15260, time: 0.527, loss: 0.074473\n",
            "step:    15280, time: 0.614, loss: 0.079560\n",
            "step:    15300, time: 0.588, loss: 0.128367\n",
            "step:    15320, time: 0.554, loss: 0.045606\n",
            "step:    15340, time: 0.588, loss: 0.081305\n",
            "step:    15360, time: 0.596, loss: 0.100404\n",
            "step:    15380, time: 0.551, loss: 0.064244\n",
            "step:    15400, time: 0.569, loss: 0.057295\n",
            "step:    15420, time: 0.660, loss: 0.063597\n",
            "step:    15440, time: 0.576, loss: 0.057078\n",
            "step:    15460, time: 0.566, loss: 0.075589\n",
            "step:    15480, time: 0.627, loss: 0.074247\n",
            "step:    15500, time: 0.596, loss: 0.044194\n",
            "step:    15520, time: 0.589, loss: 0.092747\n",
            "step:    15540, time: 0.558, loss: 0.095222\n",
            "step:    15560, time: 0.612, loss: 0.104819\n",
            "step:    15580, time: 0.581, loss: 0.075490\n",
            "step:    15600, time: 0.545, loss: 0.097628\n",
            "step:    15620, time: 0.702, loss: 0.150312\n",
            "step:    15640, time: 0.554, loss: 0.056298\n",
            "step:    15660, time: 0.621, loss: 0.103522\n",
            "step:    15680, time: 0.609, loss: 0.262294\n",
            "step:    15700, time: 0.563, loss: 0.072328\n",
            "step:    15720, time: 0.611, loss: 0.128354\n",
            "step:    15740, time: 0.620, loss: 0.078599\n",
            "step:    15760, time: 0.588, loss: 0.096806\n",
            "step:    15780, time: 0.597, loss: 0.166852\n",
            "step:    15800, time: 0.608, loss: 0.110438\n",
            "step:    15820, time: 0.605, loss: 0.108896\n",
            "step:    15840, time: 0.568, loss: 0.056648\n",
            "step:    15860, time: 0.627, loss: 0.074790\n",
            "step:    15880, time: 0.612, loss: 0.099274\n",
            "step:    15900, time: 0.673, loss: 0.068885\n",
            "step:    15920, time: 0.624, loss: 0.063186\n",
            "step:    15940, time: 0.614, loss: 0.082731\n",
            "step:    15960, time: 0.530, loss: 0.102761\n",
            "step:    15980, time: 0.609, loss: 0.132212\n",
            "step:    16000, time: 0.604, loss: 0.038611\n",
            "step:    16020, time: 0.612, loss: 0.106796\n",
            "step:    16040, time: 0.581, loss: 0.087356\n",
            "step:    16060, time: 0.582, loss: 0.059014\n",
            "step:    16080, time: 0.580, loss: 0.076529\n",
            "step:    16100, time: 0.642, loss: 0.080341\n",
            "step:    16120, time: 0.587, loss: 0.065566\n",
            "step:    16140, time: 0.628, loss: 0.079094\n",
            "step:    16160, time: 0.604, loss: 0.061266\n",
            "step:    16180, time: 0.592, loss: 0.071877\n",
            "step:    16200, time: 0.606, loss: 0.122584\n",
            "step:    16220, time: 0.557, loss: 0.102627\n",
            "step:    16240, time: 0.632, loss: 0.096801\n",
            "step:    16260, time: 0.617, loss: 0.067315\n",
            "step:    16280, time: 0.627, loss: 0.082445\n",
            "step:    16300, time: 0.640, loss: 0.121341\n",
            "step:    16320, time: 0.639, loss: 0.094843\n",
            "step:    16340, time: 0.637, loss: 0.089539\n",
            "step:    16360, time: 0.543, loss: 0.071653\n",
            "step:    16380, time: 0.594, loss: 0.096179\n",
            "step:    16400, time: 0.565, loss: 0.081656\n",
            "step:    16420, time: 0.606, loss: 0.083234\n",
            "step:    16440, time: 0.625, loss: 0.072528\n",
            "step:    16460, time: 0.597, loss: 0.106766\n",
            "step:    16480, time: 0.610, loss: 0.092412\n",
            "step:    16500, time: 0.623, loss: 0.070261\n",
            "step:    16520, time: 0.613, loss: 0.155166\n",
            "step:    16540, time: 0.562, loss: 0.072679\n",
            "step:    16560, time: 0.590, loss: 0.122983\n",
            "step:    16580, time: 0.576, loss: 0.073782\n",
            "step:    16600, time: 0.629, loss: 0.067122\n",
            "step:    16620, time: 0.587, loss: 0.080837\n",
            "step:    16640, time: 0.613, loss: 0.099545\n",
            "step:    16660, time: 0.546, loss: 0.052053\n",
            "step:    16680, time: 0.589, loss: 0.085861\n",
            "step:    16700, time: 0.556, loss: 0.087967\n",
            "step:    16720, time: 0.543, loss: 0.111542\n",
            "step:    16740, time: 0.643, loss: 0.068417\n",
            "step:    16760, time: 0.566, loss: 0.112367\n",
            "step:    16780, time: 0.554, loss: 0.136428\n",
            "step:    16800, time: 0.597, loss: 0.039483\n",
            "step:    16820, time: 0.600, loss: 0.070096\n",
            "step:    16840, time: 0.565, loss: 0.076294\n",
            "step:    16860, time: 0.568, loss: 0.047382\n",
            "step:    16880, time: 0.587, loss: 0.068178\n",
            "step:    16900, time: 0.566, loss: 0.059107\n",
            "step:    16920, time: 0.612, loss: 0.092516\n",
            "step:    16940, time: 0.571, loss: 0.060882\n",
            "step:    16960, time: 0.628, loss: 0.094115\n",
            "step:    16980, time: 0.566, loss: 0.063202\n",
            "step:    17000, time: 0.619, loss: 0.112568\n",
            "step:    17020, time: 0.636, loss: 0.136606\n",
            "step:    17040, time: 0.575, loss: 0.074515\n",
            "step:    17060, time: 0.582, loss: 0.126808\n",
            "step:    17080, time: 0.595, loss: 0.067961\n",
            "step:    17100, time: 0.632, loss: 0.097229\n",
            "step:    17120, time: 0.619, loss: 0.081037\n",
            "step:    17140, time: 0.585, loss: 0.042698\n",
            "step:    17160, time: 0.596, loss: 0.095514\n",
            "step:    17180, time: 0.571, loss: 0.069243\n",
            "step:    17200, time: 0.587, loss: 0.056538\n",
            "step:    17220, time: 0.565, loss: 0.122375\n",
            "step:    17240, time: 0.587, loss: 0.088658\n",
            "step:    17260, time: 0.528, loss: 0.101123\n",
            "step:    17280, time: 0.558, loss: 0.052541\n",
            "step:    17300, time: 0.587, loss: 0.117216\n",
            "step:    17320, time: 0.545, loss: 0.045768\n",
            "step:    17340, time: 0.576, loss: 0.090242\n",
            "step:    17360, time: 0.614, loss: 0.061771\n",
            "step:    17380, time: 0.582, loss: 0.110963\n",
            "step:    17400, time: 0.671, loss: 0.081876\n",
            "step:    17420, time: 0.589, loss: 0.077150\n",
            "step:    17440, time: 0.628, loss: 0.145071\n",
            "step:    17460, time: 0.585, loss: 0.063312\n",
            "step:    17480, time: 0.579, loss: 0.071826\n",
            "step:    17500, time: 0.655, loss: 0.075659\n",
            "step:    17520, time: 0.618, loss: 0.075213\n",
            "step:    17540, time: 0.695, loss: 0.091582\n",
            "step:    17560, time: 0.565, loss: 0.125385\n",
            "step:    17580, time: 0.589, loss: 0.072466\n",
            "step:    17600, time: 0.587, loss: 0.123690\n",
            "step:    17620, time: 0.575, loss: 0.095976\n",
            "step:    17640, time: 0.583, loss: 0.120686\n",
            "step:    17660, time: 0.553, loss: 0.046498\n",
            "step:    17680, time: 0.595, loss: 0.064157\n",
            "step:    17700, time: 0.597, loss: 0.076651\n",
            "step:    17720, time: 0.593, loss: 0.054791\n",
            "step:    17740, time: 0.667, loss: 0.075528\n",
            "step:    17760, time: 0.591, loss: 0.101433\n",
            "step:    17780, time: 0.132, loss: 0.038395\n",
            "step:    17800, time: 0.759, loss: 0.082588\n",
            "step:    17820, time: 0.568, loss: 0.097675\n",
            "step:    17840, time: 0.617, loss: 0.088461\n",
            "step:    17860, time: 0.505, loss: 0.063409\n",
            "step:    17880, time: 0.582, loss: 0.129548\n",
            "step:    17900, time: 0.668, loss: 0.080707\n",
            "step:    17920, time: 0.571, loss: 0.074347\n",
            "step:    17940, time: 0.557, loss: 0.063779\n",
            "step:    17960, time: 0.598, loss: 0.059273\n",
            "step:    17980, time: 0.593, loss: 0.081647\n",
            "step:    18000, time: 0.529, loss: 0.074554\n",
            "step:    18020, time: 0.574, loss: 0.103666\n",
            "step:    18040, time: 0.593, loss: 0.073562\n",
            "step:    18060, time: 0.675, loss: 0.101420\n",
            "step:    18080, time: 0.544, loss: 0.077836\n",
            "step:    18100, time: 0.584, loss: 0.079679\n",
            "step:    18120, time: 0.589, loss: 0.074056\n",
            "step:    18140, time: 0.580, loss: 0.062818\n",
            "step:    18160, time: 0.549, loss: 0.057219\n",
            "step:    18180, time: 0.524, loss: 0.103844\n",
            "step:    18200, time: 0.573, loss: 0.094279\n",
            "step:    18220, time: 0.600, loss: 0.105577\n",
            "step:    18240, time: 0.585, loss: 0.074203\n",
            "step:    18260, time: 0.660, loss: 0.096480\n",
            "step:    18280, time: 0.641, loss: 0.075699\n",
            "step:    18300, time: 0.581, loss: 0.099491\n",
            "step:    18320, time: 0.619, loss: 0.077663\n",
            "step:    18340, time: 0.593, loss: 0.079014\n",
            "step:    18360, time: 0.635, loss: 0.058193\n",
            "step:    18380, time: 0.622, loss: 0.146374\n",
            "step:    18400, time: 0.624, loss: 0.096526\n",
            "step:    18420, time: 0.661, loss: 0.049278\n",
            "step:    18440, time: 0.625, loss: 0.042761\n",
            "step:    18460, time: 0.600, loss: 0.056911\n",
            "step:    18480, time: 0.590, loss: 0.072421\n",
            "step:    18500, time: 0.547, loss: 0.095577\n",
            "step:    18520, time: 0.690, loss: 0.097249\n",
            "step:    18540, time: 0.571, loss: 0.054093\n",
            "step:    18560, time: 0.570, loss: 0.062025\n",
            "step:    18580, time: 0.611, loss: 0.080342\n",
            "step:    18600, time: 0.576, loss: 0.075556\n",
            "step:    18620, time: 0.629, loss: 0.071811\n",
            "step:    18640, time: 0.673, loss: 0.089282\n",
            "step:    18660, time: 0.512, loss: 0.049337\n",
            "step:    18680, time: 0.576, loss: 0.063217\n",
            "step:    18700, time: 0.609, loss: 0.071810\n",
            "step:    18720, time: 0.707, loss: 0.065365\n",
            "step:    18740, time: 0.536, loss: 0.067083\n",
            "step:    18760, time: 0.581, loss: 0.053550\n",
            "step:    18780, time: 0.610, loss: 0.079314\n",
            "step:    18800, time: 0.581, loss: 0.031874\n",
            "step:    18820, time: 0.643, loss: 0.082226\n",
            "step:    18840, time: 0.598, loss: 0.057160\n",
            "step:    18860, time: 0.555, loss: 0.062229\n",
            "step:    18880, time: 0.599, loss: 0.037838\n",
            "step:    18900, time: 0.587, loss: 0.079191\n",
            "step:    18920, time: 0.573, loss: 0.061340\n",
            "step:    18940, time: 0.623, loss: 0.083025\n",
            "step:    18960, time: 0.643, loss: 0.091324\n",
            "step:    18980, time: 0.617, loss: 0.049786\n",
            "step:    19000, time: 0.595, loss: 0.076352\n",
            "step:    19020, time: 0.578, loss: 0.086282\n",
            "step:    19040, time: 0.551, loss: 0.072948\n",
            "step:    19060, time: 0.591, loss: 0.086968\n",
            "step:    19080, time: 0.612, loss: 0.047077\n",
            "step:    19100, time: 0.530, loss: 0.066370\n",
            "step:    19120, time: 0.615, loss: 0.070509\n",
            "step:    19140, time: 0.591, loss: 0.070345\n",
            "step:    19160, time: 0.564, loss: 0.096838\n",
            "step:    19180, time: 0.626, loss: 0.101397\n",
            "step:    19200, time: 0.559, loss: 0.079430\n",
            "step:    19220, time: 0.581, loss: 0.058783\n",
            "step:    19240, time: 0.668, loss: 0.079508\n",
            "step:    19260, time: 0.589, loss: 0.090405\n",
            "step:    19280, time: 0.595, loss: 0.110587\n",
            "step:    19300, time: 0.560, loss: 0.120378\n",
            "step:    19320, time: 0.634, loss: 0.094847\n",
            "step:    19340, time: 0.604, loss: 0.120800\n",
            "step:    19360, time: 0.592, loss: 0.098634\n",
            "step:    19380, time: 0.579, loss: 0.051502\n",
            "step:    19400, time: 0.549, loss: 0.068987\n",
            "step:    19420, time: 0.561, loss: 0.137082\n",
            "step:    19440, time: 0.566, loss: 0.058781\n",
            "step:    19460, time: 0.625, loss: 0.062680\n",
            "step:    19480, time: 0.573, loss: 0.079118\n",
            "step:    19500, time: 0.573, loss: 0.071893\n",
            "step:    19520, time: 0.585, loss: 0.081423\n",
            "step:    19540, time: 0.518, loss: 0.059022\n",
            "step:    19560, time: 0.567, loss: 0.071784\n",
            "step:    19580, time: 0.553, loss: 0.054432\n",
            "step:    19600, time: 0.655, loss: 0.098097\n",
            "step:    19620, time: 0.607, loss: 0.057678\n",
            "step:    19640, time: 0.588, loss: 0.074328\n",
            "step:    19660, time: 0.587, loss: 0.065836\n",
            "step:    19680, time: 0.565, loss: 0.076181\n",
            "step:    19700, time: 0.584, loss: 0.044802\n",
            "step:    19720, time: 0.543, loss: 0.079371\n",
            "step:    19740, time: 0.600, loss: 0.078153\n",
            "step:    19760, time: 0.586, loss: 0.055297\n",
            "step:    19780, time: 0.567, loss: 0.093509\n",
            "step:    19800, time: 0.612, loss: 0.076980\n",
            "step:    19820, time: 0.602, loss: 0.090747\n",
            "step:    19840, time: 0.565, loss: 0.130090\n",
            "step:    19860, time: 0.631, loss: 0.094463\n",
            "step:    19880, time: 0.601, loss: 0.059164\n",
            "step:    19900, time: 0.567, loss: 0.075553\n",
            "step:    19920, time: 0.550, loss: 0.050868\n",
            "step:    19940, time: 0.589, loss: 0.084589\n",
            "step:    19960, time: 0.626, loss: 0.052663\n",
            "step:    19980, time: 0.591, loss: 0.055979\n",
            "step:    20000, time: 0.586, loss: 0.086868\n",
            "step:    20020, time: 0.575, loss: 0.062772\n",
            "step:    20040, time: 0.650, loss: 0.119759\n",
            "step:    20060, time: 0.593, loss: 0.090858\n",
            "step:    20080, time: 0.596, loss: 0.087746\n",
            "step:    20100, time: 0.612, loss: 0.105859\n",
            "step:    20120, time: 0.561, loss: 0.087564\n",
            "step:    20140, time: 0.572, loss: 0.099794\n",
            "step:    20160, time: 0.582, loss: 0.063342\n",
            "step:    20180, time: 0.569, loss: 0.049336\n",
            "step:    20200, time: 0.654, loss: 0.109735\n",
            "step:    20220, time: 0.623, loss: 0.117426\n",
            "step:    20240, time: 0.617, loss: 0.108655\n",
            "step:    20260, time: 0.604, loss: 0.076593\n",
            "step:    20280, time: 0.582, loss: 0.082062\n",
            "step:    20300, time: 0.649, loss: 0.060598\n",
            "step:    20320, time: 0.590, loss: 0.104444\n",
            "step:    20340, time: 0.552, loss: 0.100352\n",
            "step:    20360, time: 0.550, loss: 0.086690\n",
            "step:    20380, time: 0.607, loss: 0.048075\n",
            "step:    20400, time: 0.565, loss: 0.060025\n",
            "step:    20420, time: 0.602, loss: 0.139161\n",
            "step:    20440, time: 0.591, loss: 0.097283\n",
            "step:    20460, time: 0.613, loss: 0.085102\n",
            "step:    20480, time: 0.643, loss: 0.086903\n",
            "step:    20500, time: 0.558, loss: 0.106012\n",
            "step:    20520, time: 0.583, loss: 0.070796\n",
            "step:    20540, time: 0.627, loss: 0.061531\n",
            "step:    20560, time: 0.523, loss: 0.100856\n",
            "step:    20580, time: 0.609, loss: 0.084064\n",
            "step:    20600, time: 0.583, loss: 0.088058\n",
            "step:    20620, time: 0.619, loss: 0.104270\n",
            "step:    20640, time: 0.586, loss: 0.055439\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}